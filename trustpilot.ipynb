{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# required for scraper with vpn\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trustpilot Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid running into beeing blocked by trustpilot reduce items in website_list or max_pages\n",
    "# maximum number of review pages to scrape per website\n",
    "max_pages = 500 + 1\n",
    "\n",
    "website_list = [\n",
    "    'www.apple.com', 'www.google.com', 'www.microsoft.com', 'www.facebook.com', 'www.twitter.com',\n",
    "    'www.amazon.com', 'www.ebay.com', 'www.porkbun.com', 'www.godaddy.com', 'www.1password.com', 'nordvpn.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape reviews from page\n",
    "def get_reviews_from_page(page, website):\n",
    "    return_df = pd.DataFrame()\n",
    "    articles = page.find_all('article')\n",
    "\n",
    "    for article in articles:\n",
    "        country = article.find_next('aside').find_all('span')[-1].string\n",
    "\n",
    "        article_section = article.find_next('section')\n",
    "        stars = re.search(r'\\d+', article_section.find('img')['alt']).group()\n",
    "        date = article_section.find('time')['datetime']\n",
    "        title = article_section.find('h2').string\n",
    "        text_raw = article_section.find('p').contents\n",
    "        text = ' '.join(line.text for line in text_raw if line.text != '')\n",
    "\n",
    "        return_df = pd.concat(\n",
    "            [return_df, pd.DataFrame(\n",
    "                [{'site': website, 'date': date, 'stars': stars, 'title': title, 'text': text, 'location': country}])],\n",
    "            ignore_index=True)\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page and scrape reviews\n",
    "def scrape_reviews_for_website(url):\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(\n",
    "        f'https://www.trustpilot.com/review/' + url).text, 'lxml')\n",
    "    max_available_pages = int(\n",
    "        soup.find('a', {'name': 'pagination-button-last'}).find('span').string) + 1\n",
    "\n",
    "    # already fetched site - no need for duplicate fetching\n",
    "    result_df = pd.concat(\n",
    "        [result_df, get_reviews_from_page(soup, url)], ignore_index=True)\n",
    "\n",
    "    # Scrape up to 500 pages. If reviews don't exceed 500 Pages scrape only these\n",
    "    for index in range(2, max_available_pages if max_pages > max_available_pages else max_pages):\n",
    "        # URL is composed of www.trustpilot.com/review + the website + the page number of the reviews as the query param page\n",
    "        page = BeautifulSoup(requests.get(\n",
    "            f'https://www.trustpilot.com/review/' + url + '?page=' + str(index)).text, 'lxml')\n",
    "        result_df = pd.concat(\n",
    "            [result_df, get_reviews_from_page(page, url)], ignore_index=True)\n",
    "\n",
    "    result_df.to_csv('data/' + url + ' - Trustpilot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for website in website_list:\n",
    "    scrape_reviews_for_website(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# scraper with PIA VPN - reset of VPN connection prevents blocking\n",
    "# works on Windows ONLY and requires the PIA Desktop Client as well as a PIA subscription\n",
    "# for website in website_list:\n",
    "#\n",
    "#     subprocess.call(\n",
    "#         '\"C:\\Program Files\\Private Internet Access\\piactl.exe\" disconnect')\n",
    "#     subprocess.call(\n",
    "#         '\"C:\\Program Files\\Private Internet Access\\piactl.exe\" connect')\n",
    "#\n",
    "#     while True:\n",
    "#         time.sleep(1)\n",
    "#\n",
    "#         if re.sub('\\\\r\\\\n', '',\n",
    "#                   subprocess.run('\"C:\\Program Files\\Private Internet Access\\piactl.exe\" get connectionstate',\n",
    "#                                  stdout=subprocess.PIPE).stdout.decode('utf-8'),\n",
    "#                   ) != 'Disconnected':\n",
    "#             break\n",
    "#\n",
    "#     scrape_reviews_for_website(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Combining all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# folder should not contain previously combined data\n",
    "filenames = glob.glob(\"data/*.csv\")\n",
    "dataframes = []\n",
    "\n",
    "for file in filenames:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "result = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "del result[result.columns[0]]\n",
    "\n",
    "result.to_csv('data/all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htw_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a211c05501a9249167c1cca736772a2815a0891fa1eac7a35b0301f1901e370"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
